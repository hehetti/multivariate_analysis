{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "blKzd3m8HFLb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95c466a9-1e61-4007-a96a-a9de2302e4f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting datetime\n",
            "  Downloading datetime-6.0-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (2025.2)\n",
            "Requirement already satisfied: google in /usr/local/lib/python3.12/dist-packages (3.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Collecting zope.interface (from datetime)\n",
            "  Downloading zope_interface-8.1.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from google) (4.13.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->google) (2.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Downloading datetime-6.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zope_interface-8.1.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (264 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.7/264.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: zope.interface, datetime\n",
            "Successfully installed datetime-6.0 zope.interface-8.1.1\n"
          ]
        }
      ],
      "source": [
        "# 세팅\n",
        "!pip install numpy pandas datetime torch torchvision scikit-learn matplotlib pytz google"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from datetime import datetime\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from pytz import timezone\n",
        "import os\n",
        "from google.colab import drive\n",
        "import copy"
      ],
      "metadata": {
        "id": "Dnr1MyWLHekH"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 가져오기\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = \"/content/drive/MyDrive/4-1_다변량통계분석/final_dataset_complete.csv\"\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# drive.mount('VGG')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gJDTWVNHtMm",
        "outputId": "19a50c2c-cb98-4674-bf56-518de3ad4376"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "WINDOW_SIZE = 24\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 1e-4\n",
        "EPOCHS = 100\n",
        "PATIENCE = 10"
      ],
      "metadata": {
        "id": "Bvv7yHXXR4Uk"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.iloc[:168].isnull().values.any()) # nan 존재\n",
        "print(df.iloc[169:].isnull().values.any()) # 그다음 행부터는 nan 존재하지 않음 확인"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNuczwOe_oQq",
        "outputId": "f1a5efcc-22f3-4890-8b77-db75c0f9405b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'datetime' in df.columns: df = df.drop(columns=['datetime'])\n",
        "Y = '청주 지역공급량(Gcal)'\n",
        "\n",
        "# 1-1. 첫 168행 삭제\n",
        "df = df.iloc[168:].reset_index(drop=True)\n",
        "print(f\"Nan 존재여부: {df.isnull().values.any()}\") # 전체 데이터에 nan 있는지 확인\n",
        "\n",
        "# 1-2. Y-X 형태로\n",
        "cols = [Y] + [c for c in df.columns if c!=Y]\n",
        "data_vals = df[cols].values\n",
        "\n",
        "# 1-3. train/val 분할\n",
        "val_size = int(len(data_vals) * 0.2)\n",
        "val_raw = data_vals[:val_size]\n",
        "train_raw = data_vals[val_size:]\n",
        "\n",
        "# 1-4. scaling - leakage 방지하기 위해 분할 먼저\n",
        "scaler = StandardScaler()\n",
        "val_data = scaler.fit_transform(val_raw)\n",
        "train_data = scaler.fit_transform(train_raw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTpFTYtiSIS5",
        "outputId": "44340a90-3f36-4bf8-d439-ef41b80d4202"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nan 존재여부: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"scaled_df 모양: {data_vals.shape}; scaled_df 길이: {data_vals.size}\")\n",
        "print(f\"val_size: {val_size}\")\n",
        "print(f\"cols size: {len(cols)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hx0jPfxGbz9E",
        "outputId": "fd4a56b1-62d0-47dd-ab99-a98ac55e092a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scaled_df 모양: (43488, 503); scaled_df 길이: 21874464\n",
            "val_size: 8697\n",
            "cols size: 503\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sliding window dataset\n",
        "class SlidingWindowDataset(Dataset):\n",
        "  def __init__(self, data, window_size):\n",
        "    self.data = data\n",
        "    self.window_size = window_size\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data) - self.window_size\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # 입력 범위: idx ~ idx + window_size (과거 데이터)\n",
        "    # 타겟 시점: idx + window_size (맞춰야 할 현재 시점)\n",
        "\n",
        "    # 1. 입력 시퀀스 추출 (과거 window_size 개 + 현재 시점 1개)\n",
        "    sequence = self.data[idx : idx + self.window_size + 1].copy() # shape: (window_size + 1, features)\n",
        "\n",
        "    # 2. 정답 추출 (마지막 시점의 0번째 컬럼 = 지역공급량)\n",
        "    target = sequence[-1, 0]\n",
        "\n",
        "    # 3. 마스킹 (Masking)\n",
        "    sequence[-1,0] = 0.0\n",
        "\n",
        "    sequence_tensor = torch.tensor(sequence.transpose(), dtype=torch.float32) # PyTorch Conv1d는 (Channel, Length) 순서를 원하므로 Transpose\n",
        "    target_tensor = torch.tensor(target, dtype=torch.float32) # 결과 shape: (Features, Window_Size + 1)\n",
        "    # print(f\"sequence_tensor: {sequence_tensor.shape}\")\n",
        "    # print(f\"target_tensor: {target_tensor.shape}\")\n",
        "\n",
        "    return sequence_tensor, target_tensor"
      ],
      "metadata": {
        "id": "E01-MglJHq5T"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SlidingWindowDataset(train_data, WINDOW_SIZE)\n",
        "val_dataset = SlidingWindowDataset(val_data, WINDOW_SIZE)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "3eRmus0fVZCq"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VGG 모델\n",
        "class TimeSeriesVGG(nn.Module):\n",
        "    def __init__(self, num_features, window_len):\n",
        "        super(TimeSeriesVGG, self).__init__()\n",
        "\n",
        "        # 입력: (Batch, Features, Time_Steps)\n",
        "        # Features: 변수 개수 (500개 등)\n",
        "        # Time_Steps: window_size + 1 (25개 등)\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1\n",
        "            nn.Conv1d(in_channels=num_features, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            # 시계열 길이가 짧으면 Pooling은 조심해서 써야 함 (정보 손실)\n",
        "            # 여기서는 차원 유지를 위해 Pooling 대신 Stride를 쓰거나 생략 가능\n",
        "            # VGG 스타일을 살려 MaxPool 적용\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 2\n",
        "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 3\n",
        "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(1) # 시간 차원을 1로 압축 (Global Pooling)\n",
        "        )\n",
        "\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(128, 1) # 최종 예측값 1개\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.regressor(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "p6NN6LtCXKOo"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early Stopping 구현\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "best_model_weights = None\n",
        "loss_history = {'train':[], 'val':[]}"
      ],
      "metadata": {
        "id": "UczX_ki3MqUg"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습\n",
        "model = TimeSeriesVGG(num_features=data_vals.shape[1], window_len=WINDOW_SIZE+1).to(DEVICE)\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "print(f\"학습 시작 (Max Epochs: {EPOCHS}, Patience: {PATIENCE})\")\n",
        "\n",
        "for epoch in range(EPOCHS): # Epoch 수 조절\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE).unsqueeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    loss_history['train'].append(avg_train_loss)\n",
        "\n",
        "    # 검증\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE).unsqueeze(1)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    loss_history['val'].append(avg_val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:03d} | Train Loss: {avg_train_loss:.5f} | Val Loss: {avg_val_loss:.5f}\")\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        # 성능이 좋아졌을 때\n",
        "        best_val_loss = avg_val_loss\n",
        "        patience_counter = 0\n",
        "        # 현재 모델의 가중치를 깊은 복사(Deep Copy)로 저장해둠\n",
        "        best_model_weights = copy.deepcopy(model.state_dict())\n",
        "        # (선택) 파일로 저장하고 싶다면: torch.save(model.state_dict(), 'best_model.pth')\n",
        "        print(\"  <-- Best Model Saved\")\n",
        "    else:\n",
        "        # 성능이 안 좋아졌거나 같을 때\n",
        "        patience_counter += 1\n",
        "        print(f\"  | Patience {patience_counter}/{PATIENCE}\")\n",
        "\n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(f\"\\n[Early Stopping] {epoch+1} 에폭에서 학습 조기 종료.\")\n",
        "            break\n",
        "\n",
        "print(\"학습 완료.\")\n",
        "# 학습이 끝난 후(혹은 중단된 후), 저장해둔 최고의 가중치로 모델을 되돌립니다.\n",
        "if best_model_weights is not None:\n",
        "    model.load_state_dict(best_model_weights)\n",
        "    print(\"최적의 검증 성능을 낸 모델로 가중치를 복구했습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_oeinCCXtqv",
        "outputId": "b1fd2337-8031-4814-ebe9-1a8800cb955e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습 시작 (Max Epochs: 100, Patience: 10)\n",
            "Epoch 001 | Train Loss: 0.82563 | Val Loss: 0.63463\n",
            "  <-- Best Model Saved\n",
            "Epoch 002 | Train Loss: 0.56854 | Val Loss: 0.71805\n",
            "  | Patience 1/10\n",
            "Epoch 003 | Train Loss: 0.45031 | Val Loss: 0.73317\n",
            "  | Patience 2/10\n",
            "Epoch 004 | Train Loss: 0.39382 | Val Loss: 0.74584\n",
            "  | Patience 3/10\n",
            "Epoch 005 | Train Loss: 0.35245 | Val Loss: 0.71869\n",
            "  | Patience 4/10\n",
            "Epoch 006 | Train Loss: 0.31929 | Val Loss: 0.74215\n",
            "  | Patience 5/10\n",
            "Epoch 007 | Train Loss: 0.29621 | Val Loss: 0.70925\n",
            "  | Patience 6/10\n",
            "Epoch 008 | Train Loss: 0.28090 | Val Loss: 0.77527\n",
            "  | Patience 7/10\n",
            "Epoch 009 | Train Loss: 0.26820 | Val Loss: 0.70458\n",
            "  | Patience 8/10\n",
            "Epoch 010 | Train Loss: 0.25921 | Val Loss: 0.72881\n",
            "  | Patience 9/10\n",
            "Epoch 011 | Train Loss: 0.24659 | Val Loss: 0.70328\n",
            "  | Patience 10/10\n",
            "\n",
            "[Early Stopping] 11 에폭에서 학습 조기 종료.\n",
            "학습 완료.\n",
            "최적의 검증 성능을 낸 모델로 가중치를 복구했습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SnjVB8KjYDL7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}