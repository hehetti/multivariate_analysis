{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blKzd3m8HFLb"
      },
      "outputs": [],
      "source": [
        "# 세팅\n",
        "!pip install numpy pandas datetime torch torchvision scikit-learn matplotlib pytz google"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from datetime import datetime\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "from pytz import timezone\n",
        "import os\n",
        "from google.colab import drive\n",
        "import copy"
      ],
      "metadata": {
        "id": "Dnr1MyWLHekH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 가져오기\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = \"/content/drive/MyDrive/4-1_다변량통계분석/final_dataset_complete.csv\"\n",
        "\n",
        "# drive.mount('VGG')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gJDTWVNHtMm",
        "outputId": "9e89fde4-0ca2-48d9-df7e-d6e4c1ea0ab5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "WINDOW_SIZE = 168\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 1e-2\n",
        "EPOCHS = 100\n",
        "PATIENCE = 10\n",
        "MODEL_NAME = \"vgg_model.h5\"\n",
        "MODEL_PATH = f\"/content/drive/MyDrive/4-1_다변량통계분석/models/{MODEL_NAME}\""
      ],
      "metadata": {
        "id": "Bvv7yHXXR4Uk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(path)\n",
        "\n",
        "print(df.iloc[:168].isnull().values.any()) # nan 존재\n",
        "# print(df.iloc[165].isnull().values.any()) # 임의의 행에 nan 존재하는지 확인\n",
        "print(df.iloc[169:].isnull().values.any()) # 그다음 행부터는 nan 존재하지 않음 확인"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNuczwOe_oQq",
        "outputId": "ccb34c40-e58a-4c29-a413-7ad727397df4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(path)\n",
        "\n",
        "# 1-1. 테스트셋(2021년도) 분리\n",
        "df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "df_test = df[df['datetime'] >= '2021-01-01'].copy().reset_index(drop=True)\n",
        "df = df[df['datetime']<'2021-01-01'].reset_index(drop=True)\n",
        "print(f\"train&eval 데이터 개수: {len(df)}\")\n",
        "print(f\"test 데이터셋 개수: {len(df_test)}\")\n",
        "\n",
        "if 'datetime' in df.columns: df = df.drop(columns=['datetime']) # datetime 제거\n",
        "Y = '청주 지역공급량(Gcal)'\n",
        "\n",
        "# 1-2. 첫 168행 삭제\n",
        "df = df.iloc[168:].reset_index(drop=True)\n",
        "print(f\"Nan 존재여부: {df.isnull().values.any()}\") # 전체 데이터에 nan 있는지 확인\n",
        "\n",
        "# 1-3. Y-X 형태로\n",
        "cols = [Y] + [c for c in df.columns if c!=Y]\n",
        "data_vals = df[cols].values\n",
        "test_vals = df_test[cols].values\n",
        "# y_test = df_test.iloc[:,0].values\n",
        "# x_test = df_test.iloc[:,2:].values\n",
        "\n",
        "# 1-4. train/val 분할\n",
        "val_size = int(len(data_vals) * 0.2)\n",
        "val_raw = data_vals[:val_size]\n",
        "train_raw = data_vals[val_size:]\n",
        "\n",
        "# 1-5. scaling - leakage 방지하기 위해 분할 먼저\n",
        "scaler = StandardScaler()\n",
        "train_data = scaler.fit_transform(train_raw)\n",
        "val_data = scaler.transform(val_raw)\n",
        "test_scaled = scaler.transform(test_vals)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTpFTYtiSIS5",
        "outputId": "25a1d6ae-45e7-4380-a563-c32f7a5fbe03"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train&eval 데이터 개수: 35064\n",
            "test 데이터셋 개수: 8760\n",
            "Nan 존재여부: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "iZkvfUe7ZgMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail(10)"
      ],
      "metadata": {
        "id": "yhfgzukeYoSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"scaled_df 모양: {data_vals.shape}; scaled_df 길이: {data_vals.size}\")\n",
        "print(f\"train_raw 모양: {train_raw.shape}; train_raw 길이: {train_raw.size}\")\n",
        "\n",
        "print(f\"val_size: {val_size}\")\n",
        "print(f\"cols size: {len(cols)}\")\n",
        "print(f\"train size: {train_data.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hx0jPfxGbz9E",
        "outputId": "488400ad-39a7-4375-8a6d-b91240101824"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scaled_df 모양: (34896, 503); scaled_df 길이: 17552688\n",
            "train_raw 모양: (27917, 503); train_raw 길이: 14042251\n",
            "val_size: 6979\n",
            "cols size: 503\n",
            "train size: (27917, 503)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sliding window dataset\n",
        "class SlidingWindowDataset(Dataset):\n",
        "  def __init__(self, data, window_size):\n",
        "    self.data = data\n",
        "    self.window_size = window_size\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data) - self.window_size\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # 입력 범위: idx ~ idx + window_size (과거 데이터)\n",
        "    # 타겟 시점: idx + window_size (맞춰야 할 현재 시점)\n",
        "\n",
        "    # 1. 입력 시퀀스 추출 (과거 window_size 개 + 현재 시점 1개)\n",
        "    sequence = self.data[idx : idx + self.window_size + 1].copy() # shape: (window_size + 1, features)\n",
        "\n",
        "    # 2. 정답 추출 (마지막 시점의 0번째 컬럼 = 지역공급량)\n",
        "    target = sequence[-1, 0] # Y\n",
        "\n",
        "    # 3. 마스크 채널\n",
        "    # mask = np.ones((sequence.shape[0],1)) # (169, 1) 꼴의 행렬 [1,1,1, ..., 1]\n",
        "    # 마지막 시점 마스킹 (데이터 0, 마스크 0)\n",
        "    sequence[-1,0] = 0.0\n",
        "    # mask[-1,0] = 0.0\n",
        "    # 데이터와 마스크 합치기 (Concatenate)\n",
        "    # sequence_masked = np.concatenate([sequence, mask], axis=1) # 결과 shape: (169, feature+1)\n",
        "\n",
        "    sequence_tensor = torch.tensor(sequence.transpose(), dtype=torch.float32) # PyTorch Conv1d는 (Channel, Length) 순서를 원하므로 Transpose\n",
        "    target_tensor = torch.tensor(target, dtype=torch.float32) # 결과 shape: (Features, Window_Size + 1)\n",
        "\n",
        "    return sequence_tensor, target_tensor\n",
        "\n",
        "# VGG 모델\n",
        "class TimeSeriesVGG(nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super(TimeSeriesVGG, self).__init__()\n",
        "\n",
        "        # 입력: (Batch, Features, Time_Steps)\n",
        "        # Features: 변수 개수 (500개 등)\n",
        "        # Time_Steps: window_size + 1 (25개 등)\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1\n",
        "            nn.Conv1d(in_channels=num_features, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            # 시계열 길이가 짧으면 Pooling은 조심해서 써야 함 (정보 손실)\n",
        "            # 여기서는 차원 유지를 위해 Pooling 대신 Stride를 쓰거나 생략 가능\n",
        "            # VGG 스타일을 살려 MaxPool 적용\n",
        "            # nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 2\n",
        "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 3\n",
        "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(1) # 시간 차원을 1로 압축 (Global Pooling)\n",
        "        )\n",
        "\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(128, 1) # 최종 예측값 1개\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.regressor(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "E01-MglJHq5T"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early Stopping 구현\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "best_model_weights = None\n",
        "loss_history = {'train':[], 'val':[]}\n",
        "\n",
        "# 데이터셋 구성\n",
        "train_dataset = SlidingWindowDataset(train_data, WINDOW_SIZE)\n",
        "val_dataset = SlidingWindowDataset(val_data, WINDOW_SIZE)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# 모델 학습\n",
        "model = TimeSeriesVGG(num_features=data_vals.shape[1]).to(DEVICE)\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "print(f\"학습 시작 (Max Epochs: {EPOCHS}, Patience: {PATIENCE})\")\n",
        "\n",
        "for epoch in range(EPOCHS): # Epoch 수 조절\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE).unsqueeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    loss_history['train'].append(avg_train_loss)\n",
        "\n",
        "    # 검증\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE).unsqueeze(1)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    loss_history['val'].append(avg_val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:03d} | Train Loss: {avg_train_loss:.5f} | Val Loss: {avg_val_loss:.5f}\")\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        # 성능이 좋아졌을 때\n",
        "        best_val_loss = avg_val_loss\n",
        "        patience_counter = 0\n",
        "        # 현재 모델의 가중치를 깊은 복사(Deep Copy)로 저장해둠\n",
        "        best_model_weights = copy.deepcopy(model.state_dict())\n",
        "        # (선택) 파일로 저장하고 싶다면:\n",
        "        torch.save(model.state_dict(), MODEL_PATH)\n",
        "        print(\"  <-- Best Model Saved\")\n",
        "    else:\n",
        "        # 성능이 안 좋아졌거나 같을 때\n",
        "        patience_counter += 1\n",
        "        print(f\"  | Patience {patience_counter}/{PATIENCE}\")\n",
        "\n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(f\"\\n[Early Stopping] {epoch+1} 에폭에서 학습 조기 종료.\")\n",
        "            break\n",
        "\n",
        "print(\"학습 완료.\")\n",
        "# 학습이 끝난 후(혹은 중단된 후), 저장해둔 최고의 가중치로 모델을 되돌립니다.\n",
        "if best_model_weights is not None:\n",
        "    model.load_state_dict(best_model_weights)\n",
        "    print(\"최적의 검증 성능을 낸 모델로 가중치를 복구했습니다.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_oeinCCXtqv",
        "outputId": "d5c9afcf-aaef-4ae9-c865-981b67fefea4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습 시작 (Max Epochs: 100, Patience: 10)\n",
            "Epoch 001 | Train Loss: 0.72015 | Val Loss: 0.78732\n",
            "  <-- Best Model Saved\n",
            "Epoch 002 | Train Loss: 0.78381 | Val Loss: 0.71770\n",
            "  <-- Best Model Saved\n",
            "Epoch 003 | Train Loss: 0.87238 | Val Loss: 0.68218\n",
            "  <-- Best Model Saved\n",
            "Epoch 004 | Train Loss: 0.87108 | Val Loss: 0.67375\n",
            "  <-- Best Model Saved\n",
            "Epoch 005 | Train Loss: 0.87070 | Val Loss: 0.67184\n",
            "  <-- Best Model Saved\n",
            "Epoch 006 | Train Loss: 0.87032 | Val Loss: 0.67155\n",
            "  <-- Best Model Saved\n",
            "Epoch 007 | Train Loss: 0.87007 | Val Loss: 0.67181\n",
            "  | Patience 1/10\n",
            "Epoch 008 | Train Loss: 0.86978 | Val Loss: 0.67207\n",
            "  | Patience 2/10\n",
            "Epoch 009 | Train Loss: 0.86958 | Val Loss: 0.67227\n",
            "  | Patience 3/10\n",
            "Epoch 010 | Train Loss: 0.86942 | Val Loss: 0.67233\n",
            "  | Patience 4/10\n",
            "Epoch 011 | Train Loss: 0.86934 | Val Loss: 0.67258\n",
            "  | Patience 5/10\n",
            "Epoch 012 | Train Loss: 0.86921 | Val Loss: 0.67250\n",
            "  | Patience 6/10\n",
            "Epoch 013 | Train Loss: 0.86918 | Val Loss: 0.67272\n",
            "  | Patience 7/10\n",
            "Epoch 014 | Train Loss: 0.86914 | Val Loss: 0.67267\n",
            "  | Patience 8/10\n",
            "Epoch 015 | Train Loss: 0.86909 | Val Loss: 0.67277\n",
            "  | Patience 9/10\n",
            "Epoch 016 | Train Loss: 0.86907 | Val Loss: 0.67270\n",
            "  | Patience 10/10\n",
            "\n",
            "[Early Stopping] 16 에폭에서 학습 조기 종료.\n",
            "학습 완료.\n",
            "최적의 검증 성능을 낸 모델로 가중치를 복구했습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 전체 데이터를 한 번에 가져오기 위해 배치를 통째로 설정\n",
        "test_dataset = SlidingWindowDataset(test_scaled, WINDOW_SIZE)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# 2. 데이터 딱 한 번만 꺼내기 (iter, next 사용)\n",
        "# inputs, targets = next(iter(full_batch_loader))\n",
        "\n",
        "predictions = []\n",
        "actuals = []\n",
        "\n",
        "# 3. 모델 예측\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for inputs, targets in test_loader:\n",
        "    inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "    outputs = model(inputs)\n",
        "    predictions.extend(outputs.cpu().numpy().flatten())\n",
        "    actuals.extend(targets.cpu().numpy().flatten())\n",
        "\n",
        "# 4. MAE 계산\n",
        "predictions = np.array(predictions)\n",
        "actuals = np.array(actuals)\n",
        "\n",
        "mae = mean_absolute_error(predictions, actuals)\n",
        "\n",
        "print(f\"최종 MAE: {mae:.4f}\")\n",
        "\n",
        "# y_mean = scaler.mean_[0]\n",
        "# y_std = scaler.scale_[0]\n",
        "\n",
        "# # 공식: (예측값 * 표준편차) + 평균\n",
        "# y_pred_real = (predictions * y_std) + y_mean\n",
        "# y_true_real = (actuals * y_std) + y_mean\n",
        "\n",
        "# real_mae = mean_absolute_error(y_true_real, y_pred_real)\n",
        "# print(f\"원래 단위(Gcal) 기준 MAE: {real_mae:.4f}\")\n",
        "\n",
        "# (데이터 개수, 전체 변수 개수) 모양의 0으로 된 빈 행렬 생성\n",
        "dummy_pred = np.zeros((len(predictions), scaler.n_features_in_))\n",
        "dummy_pred_2 = np.zeros((len(predictions), scaler.n_features_in_))\n",
        "\n",
        "# 0번째 열에만 예측값 집어넣기\n",
        "dummy_pred[:, 0] = predictions\n",
        "dummy_pred_2[:, 0] = actuals\n",
        "\n",
        "# 전체를 역변환한 뒤, 다시 0번째 열만 뽑아내기\n",
        "y_pred_real = scaler.inverse_transform(dummy_pred)[:, 0]\n",
        "y_true_real = scaler.inverse_transform(dummy_pred_2)[:, 0]\n",
        "\n",
        "real_mae = mean_absolute_error(y_true_real, y_pred_real)\n",
        "print(f\"원래 단위(Gcal) 기준 MAE: {real_mae:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8w5GCZ2m66Y",
        "outputId": "8578982c-bf74-47df-ca5a-e6f636bd86be"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원래 단위(Gcal) 기준 MAE: 42.5672\n"
          ]
        }
      ]
    }
  ]
}