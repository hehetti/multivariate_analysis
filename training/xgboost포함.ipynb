{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "id": "9PU8AY9eJa0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"지금 죽지 마\")"
      ],
      "metadata": {
        "id": "7AhG5QItBr2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJuy8FmPIxfG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from catboost import CatBoostRegressor, Pool\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 1. CSV 파일 읽기 + 최근 30000개만 사용\n",
        "# -------------------------------------------------\n",
        "try:\n",
        "    del df\n",
        "    del df_test\n",
        "except:\n",
        "    pass\n",
        "\n",
        "file_path = \"/content/final_heatmap_lag_without_leakage.csv\"\n",
        "df_raw = pd.read_csv(file_path)\n",
        "\n",
        "# 실험 데이터셋(2021)과 분리\n",
        "df_raw['datetime'] = pd.to_datetime(df_raw['datetime'])\n",
        "print(f\"전체 로드된 데이터셋: {len(df_raw)}\")\n",
        "print(f\"날짜 범위: {df_raw['datetime'].min()} ~ {df_raw['datetime'].max()}\")\n",
        "\n",
        "df_test = df_raw[df_raw['datetime'] >= '2021-01-01'].copy().reset_index(drop=True)\n",
        "df = df_raw[df_raw['datetime']<'2021-01-01'].copy().reset_index(drop=True)\n",
        "\n",
        "# 첫 168행 제거\n",
        "df = df.iloc[168:].reset_index(drop=True)\n",
        "print(f\"전체 train&eval 데이터 개수: {len(df)}\")\n",
        "print(f\"test 데이터셋 개수: {len(df_test)}\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 2. 첫 번째 열이 y, 나머지 열이 X\n",
        "# -------------------------------------------------\n",
        "y = df.iloc[:, 0].values\n",
        "X = df.iloc[:, 2:].values  # 두 번째 열 제외한 나머지 feature\n",
        "\n",
        "y_2021 = df_test.iloc[:, 0].values\n",
        "X_2021 = df_test.iloc[:, 2:].values\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "\n",
        "print(\"X_2021 shape:\", X_2021.shape)\n",
        "print(\"y_2021 shape:\", y_2021.shape)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 3. Train/Test Split (앞쪽 1/4 = test)\n",
        "# -------------------------------------------------\n",
        "test_size = len(df) // 4\n",
        "\n",
        "X_train, X_test = X[test_size:], X[:test_size]\n",
        "y_train, y_test = y[test_size:], y[:test_size]\n",
        "\n",
        "scaler.fit(X_train)\n",
        "\n",
        "# train과 test 모두 transform\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_2021 = scaler.transform(X_2021)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 4. CatBoost 모델 (MAE + Early Stopping)\n",
        "# -------------------------------------------------\n",
        "model = CatBoostRegressor(\n",
        "    iterations=5000,\n",
        "    learning_rate=0.03,\n",
        "    depth=8,\n",
        "\n",
        "    # ◆◆ MAE 기준으로 변경됨 ◆◆\n",
        "    loss_function='RMSE',\n",
        "    eval_metric='MAE',\n",
        "\n",
        "    random_seed=42,\n",
        "    l2_leaf_reg=3,\n",
        "    subsample=0.8,\n",
        "    bootstrap_type='Bernoulli',\n",
        "\n",
        "    # Early stopping\n",
        "    od_type='Iter',\n",
        "    od_wait=200,\n",
        "    verbose=200\n",
        ")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 5. Fit\n",
        "# -------------------------------------------------\n",
        "train_pool = Pool(X_train, y_train)\n",
        "test_pool = Pool(X_test, y_test)\n",
        "\n",
        "model.fit(\n",
        "    train_pool,\n",
        "    eval_set=test_pool,\n",
        "    use_best_model=True\n",
        ")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 6. Predict\n",
        "# -------------------------------------------------\n",
        "y_pred = model.predict(X_2021)\n",
        "print(\"Sample predictions:\", y_pred[:5])\n",
        "mae = mean_absolute_error(y_2021, y_pred)\n",
        "print(\"MAE:\", mae)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from lightgbm import early_stopping, log_evaluation\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 1. CSV 읽기 + 최근 30000개만 사용\n",
        "# -------------------------------------------------\n",
        "try:\n",
        "    del df\n",
        "    del df_test\n",
        "except:\n",
        "    pass\n",
        "\n",
        "file_path = \"/content/final_heatmap_lag_without_leakage.csv\"\n",
        "df_raw = pd.read_csv(file_path)\n",
        "\n",
        "# 실험 데이터셋(2021)과 분리\n",
        "df_raw['datetime'] = pd.to_datetime(df_raw['datetime'])\n",
        "print(f\"전체 로드된 데이터셋: {len(df_raw)}\")\n",
        "print(f\"날짜 범위: {df_raw['datetime'].min()} ~ {df_raw['datetime'].max()}\")\n",
        "\n",
        "df_test = df_raw[df_raw['datetime'] >= '2021-01-01'].copy().reset_index(drop=True)\n",
        "df = df_raw[df_raw['datetime']<'2021-01-01'].copy().reset_index(drop=True)\n",
        "\n",
        "# 첫 168행 제거\n",
        "df = df.iloc[169:].reset_index(drop=True)\n",
        "print(f\"전체 train&eval 데이터 개수: {len(df)}\")\n",
        "print(f\"test 데이터셋 개수: {len(df_test)}\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 2. 첫 번째 열 = y, 나머지 = X\n",
        "# -------------------------------------------------\n",
        "y = df.iloc[:, 0].values\n",
        "X = df.iloc[:, 2:].values\n",
        "\n",
        "y_2021 = df_test.iloc[:, 0].values\n",
        "X_2021 = df_test.iloc[:, 2:].values\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 3. Train/Test Split (앞쪽 1/4 test)\n",
        "# -------------------------------------------------\n",
        "test_size = len(df) // 4\n",
        "X_train, X_test = X[test_size:], X[:test_size]\n",
        "y_train, y_test = y[test_size:], y[:test_size]\n",
        "\n",
        "scaler2 = StandardScaler()\n",
        "scaler2.fit(X_train)\n",
        "\n",
        "# train과 test 모두 transform\n",
        "X_train = scaler2.transform(X_train)\n",
        "X_test = scaler2.transform(X_test)\n",
        "\n",
        "X_2021_scaled = scaler2.transform(X_2021)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 4. Dataset 생성\n",
        "# -------------------------------------------------\n",
        "train_data = lgb.Dataset(X_train, label=y_train)\n",
        "valid_data = lgb.Dataset(X_test, label=y_test)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 5. 파라미터\n",
        "# -------------------------------------------------\n",
        "params = {\n",
        "    \"objective\": \"regression_l2\",   # ← MAE 기반 학습\n",
        "    \"metric\": \"l1\",\n",
        "    \"learning_rate\": 0.03,\n",
        "    \"num_leaves\": 64,\n",
        "    \"feature_fraction\": 0.9,\n",
        "    \"bagging_fraction\": 0.8,\n",
        "    \"bagging_freq\": 1,\n",
        "    \"lambda_l2\": 2.0,\n",
        "    \"random_state\": 32,\n",
        "}\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 6. Train (callback 기반 early stopping)\n",
        "# -------------------------------------------------\n",
        "model = lgb.train(\n",
        "    params,\n",
        "    train_data,\n",
        "    num_boost_round=5000,\n",
        "    valid_sets=[valid_data],\n",
        "\n",
        "    # ← early stopping 처리\n",
        "    callbacks=[\n",
        "        early_stopping(stopping_rounds=200),  # 200회 개선 없으면 stop\n",
        "        log_evaluation(200)                   # 200 iteration마다 로그 출력\n",
        "    ]\n",
        ")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 7. Predict\n",
        "# -------------------------------------------------\n",
        "y_pred = model.predict(X_2021_scaled)\n",
        "print(\"Sample predictions:\", y_pred[:5])\n",
        "mae = mean_absolute_error(y_2021, y_pred)\n",
        "print(\"MAE:\", mae)\n"
      ],
      "metadata": {
        "id": "cRPLe4zdYh9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_absolute_error  # 이미 위에 있으면 중복 import는 무시됨\n",
        "\n",
        "# -------------------------------------------------\n",
        "# XGBoost용 DMatrix 생성 (위에서 만든 X_train, X_test, X_2021_scaled 사용)\n",
        "# -------------------------------------------------\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dvalid = xgb.DMatrix(X_test,  label=y_test)\n",
        "d2021  = xgb.DMatrix(X_2021_scaled)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# XGBoost 파라미터 (LightGBM과 비슷하게 맞춤)\n",
        "# -------------------------------------------------\n",
        "params_xgb = {\n",
        "    \"objective\": \"reg:squarederror\",  # 회귀 목적함수\n",
        "    \"eval_metric\": \"mae\",             # MAE 기준으로 early stopping\n",
        "    \"eta\": 0.03,                      # learning_rate\n",
        "    \"max_depth\": 6,                   # num_leaves 비슷한 복잡도\n",
        "    \"subsample\": 0.8,                 # bagging_fraction 유사\n",
        "    \"colsample_bytree\": 0.9,          # feature_fraction 유사\n",
        "    \"lambda\": 2.0,                    # L2 정규화 계수\n",
        "    \"tree_method\": \"hist\",            # 빠른 학습 (GPU 쓰면 \"gpu_hist\")\n",
        "    \"random_state\": 32,\n",
        "}\n",
        "\n",
        "# -------------------------------------------------\n",
        "# XGBoost Train + Early Stopping\n",
        "# -------------------------------------------------\n",
        "evals = [(dtrain, \"train\"), (dvalid, \"valid\")]\n",
        "\n",
        "model_xgb = xgb.train(\n",
        "    params=params_xgb,\n",
        "    dtrain=dtrain,\n",
        "    num_boost_round=5000,        # 최대 트리 개수\n",
        "    evals=evals,                 # 검증 세트 지정\n",
        "    early_stopping_rounds=200,   # 200번 동안 MAE 개선 없으면 stop\n",
        "    verbose_eval=200             # 200 iteration마다 로그 출력\n",
        ")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 2021 데이터에 대한 예측 + MAE 계산\n",
        "# -------------------------------------------------\n",
        "y_pred_2021_xgb = model_xgb.predict(d2021, iteration_range=(0, model_xgb.best_iteration + 1))\n",
        "\n",
        "print(\"Sample XGBoost predictions:\", y_pred_2021_xgb[:5])\n",
        "\n",
        "mae_xgb = mean_absolute_error(y_2021, y_pred_2021_xgb)\n",
        "print(\"XGBoost MAE:\", mae_xgb)\n"
      ],
      "metadata": {
        "id": "zVSqns6ucR_b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}