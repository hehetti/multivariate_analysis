{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "id": "9PU8AY9eJa0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJuy8FmPIxfG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from catboost import CatBoostRegressor, Pool\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 1. CSV 파일 읽기 + 최근 30000개만 사용\n",
        "# -------------------------------------------------\n",
        "file_path = \"/content/final_heatmap_lag.csv\"\n",
        "sd = pd.read_csv(file_path)\n",
        "\n",
        "# 최근 35064개 사용\n",
        "df = sd.head(35064).reset_index(drop=True)\n",
        "# 최근 168행 제거\n",
        "df = df.iloc[168:].reset_index(drop=True)\n",
        "sd = sd.tail(8760)\n",
        "y_t = sd.iloc[:, 0].values\n",
        "X_t = sd.iloc[:, 2:].values\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 2. 첫 번째 열이 y, 나머지 열이 X\n",
        "# -------------------------------------------------\n",
        "y = df.iloc[:, 0].values\n",
        "X = df.iloc[:, 2:].values  # 두 번째 열 제외한 나머지 feature\n",
        "\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 3. Train/Test Split (앞쪽 1/4 = test)\n",
        "# -------------------------------------------------\n",
        "test_size = len(df) // 4\n",
        "\n",
        "X_train, X_test = X[test_size:], X[:test_size]\n",
        "y_train, y_test = y[test_size:], y[:test_size]\n",
        "\n",
        "scaler.fit(X_train)\n",
        "\n",
        "# train과 test 모두 transform\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 4. CatBoost 모델 (MAE + Early Stopping)\n",
        "# -------------------------------------------------\n",
        "model = CatBoostRegressor(\n",
        "    iterations=2000,\n",
        "    learning_rate=0.03,\n",
        "    depth=8,\n",
        "\n",
        "    # ◆◆ MAE 기준으로 변경됨 ◆◆\n",
        "    loss_function='MAE',\n",
        "    eval_metric='MAE',\n",
        "\n",
        "    random_seed=42,\n",
        "    l2_leaf_reg=3,\n",
        "    subsample=0.8,\n",
        "    bootstrap_type='Bernoulli',\n",
        "\n",
        "    # Early stopping\n",
        "    od_type='Iter',\n",
        "    od_wait=200,\n",
        "    verbose=200\n",
        ")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 5. Fit\n",
        "# -------------------------------------------------\n",
        "train_pool = Pool(X_train, y_train)\n",
        "test_pool = Pool(X_test, y_test)\n",
        "\n",
        "model.fit(\n",
        "    train_pool,\n",
        "    eval_set=test_pool,\n",
        "    use_best_model=True\n",
        ")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 6. Predict\n",
        "# -------------------------------------------------\n",
        "X_t = scaler.transform(X_t)\n",
        "y_pred = model.predict(X_t)\n",
        "\n",
        "print(\"Sample predictions:\", y_pred[:5])\n",
        "\n",
        "mae = mean_absolute_error(y_t, y_pred)\n",
        "print(\"Test MAE (last 15% of data):\", mae)\n",
        "\n",
        "model.save_model(\"/content/catboost_model.cbm\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from lightgbm import early_stopping, log_evaluation\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 1. CSV 읽기 + 최근 30000개만 사용\n",
        "# -------------------------------------------------\n",
        "file_path = \"/content/final_dataset_complete.csv\"\n",
        "sd = pd.read_csv(file_path)\n",
        "df = sd.head(35064).reset_index(drop=True)\n",
        "df = df.iloc[168:].reset_index(drop=True)\n",
        "\n",
        "sd = sd.tail(8760)\n",
        "y_t = sd.iloc[:, 0].values\n",
        "X_t = sd.iloc[:, 2:].values\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 2. 첫 번째 열 = y, 나머지 = X\n",
        "# -------------------------------------------------\n",
        "y = df.iloc[:, 0].values\n",
        "X = df.iloc[:, 2:].values\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 3. Train/Test Split (앞쪽 1/4 test)\n",
        "# -------------------------------------------------\n",
        "test_size = len(df) // 4\n",
        "X_train, X_test = X[test_size:], X[:test_size]\n",
        "y_train, y_test = y[test_size:], y[:test_size]\n",
        "\n",
        "scaler2 = StandardScaler()\n",
        "scaler2.fit(X_train)\n",
        "\n",
        "# train과 test 모두 transform\n",
        "X_train = scaler2.transform(X_train)\n",
        "X_test = scaler2.transform(X_test)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 4. Dataset 생성\n",
        "# -------------------------------------------------\n",
        "train_data = lgb.Dataset(X_train, label=y_train)\n",
        "valid_data = lgb.Dataset(X_test, label=y_test)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 5. 파라미터\n",
        "# -------------------------------------------------\n",
        "params = {\n",
        "    \"objective\": \"regression_l1\",   # ← MAE 기반 학습\n",
        "    \"metric\": \"l1\",\n",
        "    \"learning_rate\": 0.03,\n",
        "    \"num_leaves\": 64,\n",
        "    \"feature_fraction\": 0.9,\n",
        "    \"bagging_fraction\": 0.8,\n",
        "    \"bagging_freq\": 1,\n",
        "    \"lambda_l2\": 2.0,\n",
        "    \"random_state\": 42,\n",
        "}\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 6. Train (callback 기반 early stopping)\n",
        "# -------------------------------------------------\n",
        "model = lgb.train(\n",
        "    params,\n",
        "    train_data,\n",
        "    num_boost_round=5000,\n",
        "    valid_sets=[valid_data],\n",
        "\n",
        "    # ← early stopping 처리\n",
        "    callbacks=[\n",
        "        early_stopping(stopping_rounds=200),  # 200회 개선 없으면 stop\n",
        "        log_evaluation(200)                   # 200 iteration마다 로그 출력\n",
        "    ]\n",
        ")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 7. Predict\n",
        "# -------------------------------------------------\n",
        "X_t = scaler.transform(X_t)\n",
        "y_pred = model.predict(X_t)\n",
        "\n",
        "print(\"Sample predictions:\", y_pred[:5])\n",
        "\n",
        "mae = mean_absolute_error(y_t, y_pred)\n",
        "print(\"Test MAE (last 15% of data):\", mae)\n",
        "\n",
        "model.save_model(\"/content/lightgbm_model.cbm\")"
      ],
      "metadata": {
        "id": "cRPLe4zdYh9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# =========================\n",
        "# 설정\n",
        "# =========================\n",
        "WINDOW_SIZE = 168\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 400\n",
        "LEARNING_RATE = 1e-3\n",
        "PATIENCE = 20        # early stopping patience\n",
        "MIN_DELTA = 1e-4       # 최소 개선량\n",
        "\n",
        "MODEL_PATH = \"/content/best_cnn_lstm.pt\"\n",
        "SCALER_PATH = \"/content/scaler.pkl\"\n",
        "\n",
        "DATA_CSV = \"/content/final_dataset_complete.csv\"  # 전체가 합쳐진 csv 파일 이름으로 바꿔줘\n",
        "\n",
        "# scaler를 fit할 구간: 앞에서 (35064 - 168)번째 행까지\n",
        "SCALER_FIT_END = 35064 - 168   # = 34896\n",
        "\n",
        "# 마지막 8760개 샘플을 test set으로 사용\n",
        "NUM_TEST_SAMPLES = 8760\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Dataset / Model / EarlyStopping 정의\n",
        "# =========================\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(-1)  # (N, 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "class CNNLSTM(nn.Module):\n",
        "    def __init__(self, num_features, cnn_channels=64, lstm_hidden=64,\n",
        "                 lstm_layers=1, dropout=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        # Conv1d: (batch, channels, seq_len)\n",
        "        self.conv1 = nn.Conv1d(\n",
        "            in_channels=num_features,\n",
        "            out_channels=cnn_channels,\n",
        "            kernel_size=3,\n",
        "            padding=1\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm1d(cnn_channels)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        # LSTM: 입력 차원은 conv 출력 채널 수\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=cnn_channels,\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(lstm_hidden, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, num_features)\n",
        "        x = x.transpose(1, 2)     # (batch, num_features, seq_len)\n",
        "\n",
        "        x = self.conv1(x)         # (batch, cnn_channels, seq_len)\n",
        "        x = self.bn1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.pool(x)          # (batch, cnn_channels, seq_len')\n",
        "\n",
        "        x = x.transpose(1, 2)     # (batch, seq_len', cnn_channels)\n",
        "\n",
        "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
        "        last_hidden = h_n[-1]     # (batch, hidden)\n",
        "\n",
        "        out = self.dropout(last_hidden)\n",
        "        out = self.fc(out)        # (batch, 1)\n",
        "        return out\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=PATIENCE, min_delta=MIN_DELTA):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = np.inf\n",
        "        self.counter = 0\n",
        "\n",
        "    def step(self, val_loss, model):\n",
        "        improved = val_loss < self.best_loss - self.min_delta\n",
        "\n",
        "        if improved:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            torch.save(model.state_dict(), MODEL_PATH)\n",
        "            print(f\"  -> Validation loss improved. Model saved to {MODEL_PATH}\")\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            print(f\"  -> No improvement. EarlyStopping counter = {self.counter}/{self.patience}\")\n",
        "\n",
        "        return self.counter >= self.patience\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 윈도우 생성 함수\n",
        "# =========================\n",
        "def make_sequences(X_matrix, y_array, window_size):\n",
        "    \"\"\"\n",
        "    X_matrix: (M, num_features)\n",
        "    y_array : (M,)  - 각 시점에 대응하는 타깃 y\n",
        "    window_size: 168\n",
        "\n",
        "    리턴:\n",
        "        X_seq: (num_samples, window_size, num_features)\n",
        "        y_seq: (num_samples,)\n",
        "    \"\"\"\n",
        "    M = X_matrix.shape[0]\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "\n",
        "    # end_idx: 윈도우의 마지막 시간 인덱스\n",
        "    for end_idx in range(window_size - 1, M):\n",
        "        start_idx = end_idx - window_size + 1\n",
        "        window = X_matrix[start_idx:end_idx + 1, :]   # (window_size, num_features)\n",
        "        target = y_array[end_idx]                    # 마지막 시점의 y\n",
        "\n",
        "        X_list.append(window)\n",
        "        y_list.append(target)\n",
        "\n",
        "    X_seq = np.stack(X_list, axis=0)\n",
        "    y_seq = np.array(y_list)\n",
        "    return X_seq, y_seq\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 메인\n",
        "# =========================\n",
        "def main():\n",
        "    # ---------- 1. 데이터 로드 ----------\n",
        "    df = pd.read_csv(DATA_CSV)\n",
        "    col_to_drop = df.columns[1]\n",
        "\n",
        "    # 그 컬럼을 드롭\n",
        "    df = df.drop(columns=col_to_drop)\n",
        "    print(\"Original data shape:\", df.shape)  # (N, num_cols)\n",
        "    num_rows, num_cols = df.shape\n",
        "\n",
        "    if SCALER_FIT_END > num_rows:\n",
        "        raise ValueError(f\"SCALER_FIT_END({SCALER_FIT_END})가 전체 행 개수({num_rows})보다 큼.\")\n",
        "\n",
        "    # ---------- 2. StandardScaler fit & transform ----------\n",
        "    # 앞에서 (35064-168)번째까지의 행으로 fit\n",
        "    fit_df = df.iloc[:SCALER_FIT_END]   # 0 ~ SCALER_FIT_END-1\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(fit_df.values)\n",
        "\n",
        "    # 전체 데이터에 스케일러 적용\n",
        "    scaled_values = scaler.transform(df.values)            # (N, num_cols)\n",
        "    joblib.dump(scaler, SCALER_PATH)\n",
        "    print(f\"Scaler fitted on first {SCALER_FIT_END} rows and saved to {SCALER_PATH}\")\n",
        "\n",
        "    # ---------- 3. y 생성 ----------\n",
        "    # 첫 번째 열을 카피해서 첫 행만 빼고 y로 사용\n",
        "    y_all = scaled_values[:, 0]        # (N,)\n",
        "    y_all = y_all[1:]                  # (N-1,)\n",
        "    M = y_all.shape[0]\n",
        "\n",
        "    # ---------- 4. X 행렬 만들기 ----------\n",
        "    # 첫 번째 열: 마지막 행 삭제 → (N-1, 1)\n",
        "    col0 = scaled_values[:-1, 0:1]\n",
        "\n",
        "    # 나머지 열: 첫 번째 행 삭제 → (N-1, num_cols-1)\n",
        "    others = scaled_values[1:, 1:]     # row 1~N-1, col 1~end\n",
        "\n",
        "    # 두 부분을 합쳐 X_matrix: (N-1, num_cols)\n",
        "    X_matrix = np.concatenate([col0, others], axis=1)\n",
        "    assert X_matrix.shape[0] == M, \"X와 y의 길이가 맞지 않습니다.\"\n",
        "\n",
        "    print(\"After shift, X_matrix shape:\", X_matrix.shape)  # (N-1, num_cols)\n",
        "    print(\"After shift, y_all shape    :\", y_all.shape)     # (N-1,)\n",
        "\n",
        "    # ---------- 5. 윈도우(168)로 시퀀스 생성 ----------\n",
        "    X_seq, y_seq = make_sequences(X_matrix, y_all, WINDOW_SIZE)\n",
        "    num_samples, T, F = X_seq.shape\n",
        "    print(\"Sequence X shape:\", X_seq.shape)  # (num_samples, 168, num_features)\n",
        "    print(\"Sequence y shape:\", y_seq.shape)  # (num_samples,)\n",
        "\n",
        "    if num_samples <= NUM_TEST_SAMPLES:\n",
        "        raise ValueError(f\"윈도우로 만든 샘플 수({num_samples})가 NUM_TEST_SAMPLES({NUM_TEST_SAMPLES})보다 적음.\")\n",
        "\n",
        "    # ---------- 6. Train/Val/Test 분할 ----------\n",
        "    # 마지막 8760개를 test set\n",
        "    X_test = X_seq[-NUM_TEST_SAMPLES:]\n",
        "    y_test = y_seq[-NUM_TEST_SAMPLES:]\n",
        "\n",
        "    X_trainval = X_seq[:-NUM_TEST_SAMPLES]\n",
        "    y_trainval = y_seq[:-NUM_TEST_SAMPLES]\n",
        "\n",
        "    n_trainval = X_trainval.shape[0]\n",
        "    n_val = n_trainval // 4  # 맨 처음 1/4을 validation\n",
        "\n",
        "    X_val = X_trainval[:n_val]\n",
        "    y_val = y_trainval[:n_val]\n",
        "\n",
        "    X_train = X_trainval[n_val:]\n",
        "    y_train = y_trainval[n_val:]\n",
        "\n",
        "    print(\"Train samples:\", X_train.shape[0])\n",
        "    print(\"Val samples  :\", X_val.shape[0])\n",
        "    print(\"Test samples :\", X_test.shape[0])\n",
        "\n",
        "    # ---------- 7. Dataset / DataLoader ----------\n",
        "    train_dataset = TimeSeriesDataset(X_train, y_train)\n",
        "    val_dataset   = TimeSeriesDataset(X_val, y_val)\n",
        "    test_dataset  = TimeSeriesDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False)\n",
        "    test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # ---------- 8. 모델/손실함수/옵티마이저 ----------\n",
        "    num_features = F\n",
        "    model = CNNLSTM(num_features=num_features).to(DEVICE)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    early_stopping = EarlyStopping(patience=PATIENCE, min_delta=MIN_DELTA)\n",
        "\n",
        "    # ---------- 9. 학습 루프 (Early Stopping 포함) ----------\n",
        "    for epoch in range(1, NUM_EPOCHS + 1):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch = X_batch.to(DEVICE)\n",
        "            y_batch = y_batch.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        avg_train_loss = np.mean(train_losses)\n",
        "\n",
        "        # ----- validation -----\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in val_loader:\n",
        "                X_batch = X_batch.to(DEVICE)\n",
        "                y_batch = y_batch.to(DEVICE)\n",
        "\n",
        "                outputs = model(X_batch)\n",
        "                loss = criterion(outputs, y_batch)\n",
        "                val_losses.append(loss.item())\n",
        "\n",
        "        avg_val_loss = np.mean(val_losses)\n",
        "        print(f\"[Epoch {epoch:03d}] Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}\")\n",
        "\n",
        "        # Early stopping 체크 (개선되면 모델 저장)\n",
        "        stop = early_stopping.step(avg_val_loss, model)\n",
        "        if stop:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    # ---------- 10. 베스트 모델 로드 후 Test MSE ----------\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "        print(f\"Best model loaded from {MODEL_PATH}\")\n",
        "    else:\n",
        "        print(\"Warning: MODEL_PATH not found. Using last epoch model.\")\n",
        "\n",
        "    model.eval()\n",
        "    test_losses = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            X_batch = X_batch.to(DEVICE)\n",
        "            y_batch = y_batch.to(DEVICE)\n",
        "\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            test_losses.append(loss.item())\n",
        "\n",
        "    avg_test_loss = np.mean(test_losses)\n",
        "    print(f\"Final Test MSE: {avg_test_loss:.6f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "_7foXJtrEsO7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}